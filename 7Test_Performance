# prompt: Calculate and Display the Test Performance

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test_tensor)
    _, predicted = torch.max(outputs.data, 1)
    test_accuracy = accuracy_score(y_test_tensor.numpy(), predicted.numpy())
    print(f'Test Accuracy: {test_accuracy:.4f}')

# prompt: Extract the Embeddings after Training. Do not generate word cloud or PCA visualization.

# Get the embeddings after training
embeddings_after_training = model.embedding.weight.detach().numpy()
print(f"Embeddings shape after training: {embeddings_after_training.shape}")

# prompt: Reduce the Embeddings using PCA

# Apply PCA to reduce the dimensionality of the embeddings
pca = PCA(n_components=2)  # Reduce to 2 components
reduced_embeddings = pca.fit_transform(embeddings_after_training)

# Print the shape of the reduced embeddings
print(f"Reduced embeddings shape: {reduced_embeddings.shape}")

# prompt: visualize top_words using an appropriate plot using the embeddings after training

# Create a scatter plot
plt.figure(figsize=(8, 6))
for i in range(len(top_10_words)):
  plt.scatter(reduced_embeddings[vocab[top_10_words[i]], 0], reduced_embeddings[vocab[top_10_words[i]], 1], label=top_10_words[i])

plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("Top 10 Words Visualization After Training (Reduced)")
plt.legend()
plt.show()

# prompt: Take 5 words each from spam and ham. Plot them using the trained embeddings. Use appropriate legends and also add text in the plot.

# Get the word frequencies from the word cloud
word_counts = wordcloud.process_text(text_data)

# Get the 10 most frequent words for spam and ham
spam_words = [word for word, count in Counter({k: v for k, v in word_counts.items() if df[df['v2'].str.contains(k)]['v1'].sum() > 0}).most_common(5)]
ham_words = [word for word, count in Counter({k: v for k, v in word_counts.items() if df[df['v2'].str.contains(k)]['v1'].sum() == 0}).most_common(5)]

# Get the word vectors for the top 5 words for spam and ham
spam_word_vectors = [embeddings_after_training[vocab[word]] for word in spam_words if word in vocab]
ham_word_vectors = [embeddings_after_training[vocab[word]] for word in ham_words if word in vocab]

# Create a PCA model with 2 components
pca = PCA(n_components=2)

# Fit the PCA model to the word vectors
spam_pca_result = pca.fit_transform(spam_word_vectors)
ham_pca_result = pca.fit_transform(ham_word_vectors)

# Create a scatter plot
plt.figure(figsize=(8, 6))
for i in range(len(spam_words)):
  plt.scatter(spam_pca_result[i, 0], spam_pca_result[i, 1], label=spam_words[i], color='red')
  plt.text(spam_pca_result[i, 0], spam_pca_result[i, 1], spam_words[i], fontsize=9)

for i in range(len(ham_words)):
  plt.scatter(ham_pca_result[i, 0], ham_pca_result[i, 1], label=ham_words[i], color='green')
  plt.text(ham_pca_result[i, 0], ham_pca_result[i, 1], ham_words[i], fontsize=9)

plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("Top 5 Words for Spam and Ham Visualization After Training")
plt.legend()
plt.show()
